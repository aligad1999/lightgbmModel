def generate_risk_dataset(n_samples=10000, n_features=82, n_categorical=10, imbalance_ratio=0.15):
    """
    Generate a realistic risk prediction dataset with time series structure
    
    Parameters:
    - n_samples: Number of samples
    - n_features: Total number of features (82)
    - n_categorical: Number of categorical features (10)
    - imbalance_ratio: Ratio of positive class (default 15% for imbalanced data)
    """
    np.random.seed(42)
    
    # Generate time index for time series structure
    dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')
    
    # Generate numerical features (72 features)
    n_numerical = n_features - n_categorical
    
    # Financial/Risk related features
    numerical_data = {
        # Financial metrics
        'income': np.random.lognormal(10, 1, n_samples),
        'debt_to_income': np.random.beta(2, 5, n_samples),
        'credit_utilization': np.random.beta(1, 3, n_samples),
        'payment_history_score': np.random.normal(750, 100, n_samples),
        'account_age_months': np.random.exponential(50, n_samples),
        'number_of_accounts': np.random.poisson(8, n_samples),
        'recent_inquiries': np.random.poisson(2, n_samples),
        'loan_amount': np.random.lognormal(9, 1, n_samples),
        'employment_length': np.random.exponential(5, n_samples),
        'annual_income': np.random.lognormal(11, 0.5, n_samples),
    }
    
    # Add more numerical features to reach 72
    for i in range(10, n_numerical):
        if i % 3 == 0:
            # Some correlated features
            numerical_data[f'feature_{i}'] = (
                numerical_data['income'] * np.random.normal(0.5, 0.1, n_samples) + 
                np.random.normal(0, 1, n_samples)
            )
        elif i % 3 == 1:
            # Time-dependent features
            trend = np.linspace(0, 1, n_samples)
            seasonal = np.sin(2 * np.pi * np.arange(n_samples) / 365)
            numerical_data[f'feature_{i}'] = (
                trend + 0.5 * seasonal + np.random.normal(0, 0.3, n_samples)
            )
        else:
            # Random features
            numerical_data[f'feature_{i}'] = np.random.normal(0, 1, n_samples)
    
    # Generate categorical features
    categorical_data = {
        'employment_type': np.random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed'], n_samples, p=[0.6, 0.2, 0.15, 0.05]),
        'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.25, 0.05]),
        'marital_status': np.random.choice(['Single', 'Married', 'Divorced'], n_samples, p=[0.4, 0.5, 0.1]),
        'housing_status': np.random.choice(['Rent', 'Own', 'Mortgage'], n_samples, p=[0.3, 0.4, 0.3]),
        'state': np.random.choice([f'State_{i}' for i in range(10)], n_samples),
        'industry': np.random.choice(['Tech', 'Finance', 'Healthcare', 'Retail', 'Manufacturing', 'Other'], n_samples),
        'loan_purpose': np.random.choice(['Personal', 'Auto', 'Home', 'Business', 'Education'], n_samples),
        'bank_relationship': np.random.choice(['New', 'Existing_1-2yr', 'Existing_3-5yr', 'Existing_5+yr'], n_samples),
        'payment_method': np.random.choice(['Auto', 'Manual', 'Mixed'], n_samples),
        'risk_segment': np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.5, 0.3, 0.2])
    }
    
    # Combine all features
    data = {**numerical_data, **categorical_data}
    df = pd.DataFrame(data)
    df['date'] = dates
    
    # Generate target variable based on features (realistic risk factors)
    risk_score = (
        -0.3 * (df['payment_history_score'] - 750) / 100 +  # Lower credit score = higher risk
        0.5 * df['debt_to_income'] +                        # Higher debt ratio = higher risk
        0.3 * df['credit_utilization'] +                    # Higher utilization = higher risk
        0.2 * np.log(df['recent_inquiries'] + 1) +          # More inquiries = higher risk
        -0.1 * np.log(df['employment_length'] + 1) +        # Longer employment = lower risk
        0.1 * (df['employment_type'] == 'Unemployed').astype(int) +
        np.random.normal(0, 0.5, n_samples)                 # Add noise
    )
    
    # Convert to binary with specified imbalance ratio
    threshold = np.percentile(risk_score, (1 - imbalance_ratio) * 100)
    df['target'] = (risk_score > threshold).astype(int)
    
    # Sort by date to maintain time series structure
    df = df.sort_values('date').reset_index(drop=True)
    
    return df

# Generate the dataset
print("Generating simulated risk dataset...")
df = generate_risk_dataset(n_samples=10000, n_features=82, n_categorical=10, imbalance_ratio=0.15)

print(f"Dataset shape: {df.shape}")
print(f"Target distribution: {df['target'].value_counts().sort_index()}")
print(f"Target imbalance ratio: {df['target'].mean():.3f}")
print(f"\nFeature types:")
print(f"- Numerical features: {df.select_dtypes(include=[np.number]).shape[1] - 1}")  # -1 for target
print(f"- Categorical features: {df.select_dtypes(include=['object']).shape[1] - 1}")  # -1 for date## 2. Data Generation (Simulated Risk Dataset)

Since you haven't provided the actual dataset, I'll create a realistic simulated dataset with your specifications.# Core libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Machine Learning libraries
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, precision_recall_curve, f1_score, accuracy_score,
    precision_score, recall_score
)

# Imbalanced learning
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import EditedNearestNeighbours

# Hyperparameter optimization
import optuna
from optuna.integration import LightGBMPruningCallback

# Utilities
from datetime import datetime
import joblib
import os

# Set random seed for reproducibility
np.random.seed(42)

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("Libraries imported successfully!")
print(f"LightGBM version: {lgb.__version__}")
print(f"Optuna version: {optuna.__version__}")## 1. Import Libraries and Setup# Risk Prediction Model using LightGBM

This notebook implements a comprehensive risk prediction model with:
- **LightGBM** for gradient boosting
- **SMOTEENN** for handling imbalanced data
- **PCA** for dimensionality reduction
- **TimeSeriesSplit** for cross-validation
- **Optuna** for hyperparameter tuning

**Dataset characteristics:**
- 82 total features (72 numerical + 10 categorical)
- Imbalanced target variable
- Time series structure