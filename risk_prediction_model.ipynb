def create_preprocessing_pipeline(categorical_features, numerical_features):
    """
    Create preprocessing pipeline for both categorical and numerical features
    """
    
    # Preprocessing for numerical features
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Preprocessing for categorical features
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
    ])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='drop'  # Drop case_open_date and other non-feature columns
    )
    
    return preprocessor

# Create preprocessing pipeline
preprocessor = create_preprocessing_pipeline(categorical_features, numerical_features)

# Prepare feature columns (excluding target and non-feature columns)
feature_columns = categorical_features + numerical_features
exclude_columns = ['person_id', 'case_id', 'case_open_date', 'isRisk']
feature_columns = [col for col in feature_columns if col in df_processed.columns and col not in exclude_columns]

print(f"Feature columns ({len(feature_columns)}): {feature_columns}")

# Prepare train, validation, and test sets
X_train = train_data[feature_columns]
y_train = train_data['isRisk']

X_val = val_data[feature_columns]
y_val = val_data['isRisk']

X_test = test_data[feature_columns]
y_test = test_data['isRisk']

print(f"\nTraining set: X={X_train.shape}, y={y_train.shape}")
print(f"Validation set: X={X_val.shape}, y={y_val.shape}")
print(f"Test set: X={X_test.shape}, y={y_test.shape}")

# Fit preprocessor and transform data
X_train_processed = preprocessor.fit_transform(X_train)
X_val_processed = preprocessor.transform(X_val)
X_test_processed = preprocessor.transform(X_test)

print(f"\nProcessed shapes:")
print(f"Train: {X_train_processed.shape}")
print(f"Validation: {X_val_processed.shape}")
print(f"Test: {X_test_processed.shape}")## 5. Feature Engineering and Preprocessing Pipelinedef temporal_split(df, date_column='case_open_date', train_ratio=0.7, val_ratio=0.15):
    """
    Perform temporal split for time series data
    """
    # Sort by date
    df_sorted = df.sort_values(date_column).reset_index(drop=True)
    
    n_samples = len(df_sorted)
    train_size = int(n_samples * train_ratio)
    val_size = int(n_samples * val_ratio)
    
    # Split indices
    train_idx = list(range(train_size))
    val_idx = list(range(train_size, train_size + val_size))
    test_idx = list(range(train_size + val_size, n_samples))
    
    # Create splits
    train_data = df_sorted.iloc[train_idx]
    val_data = df_sorted.iloc[val_idx]
    test_data = df_sorted.iloc[test_idx]
    
    print(f"Temporal Split Results:")
    print(f"Train set: {len(train_data)} samples ({len(train_data)/len(df_sorted)*100:.1f}%)")
    print(f"  Date range: {train_data[date_column].min()} to {train_data[date_column].max()}")
    print(f"  Risk rate: {train_data['isRisk'].mean():.3f}")
    
    print(f"Validation set: {len(val_data)} samples ({len(val_data)/len(df_sorted)*100:.1f}%)")
    print(f"  Date range: {val_data[date_column].min()} to {val_data[date_column].max()}")
    print(f"  Risk rate: {val_data['isRisk'].mean():.3f}")
    
    print(f"Test set: {len(test_data)} samples ({len(test_data)/len(df_sorted)*100:.1f}%)")
    print(f"  Date range: {test_data[date_column].min()} to {test_data[date_column].max()}")
    print(f"  Risk rate: {test_data['isRisk'].mean():.3f}")
    
    return train_data, val_data, test_data

# Perform temporal split
train_data, val_data, test_data = temporal_split(df_processed)## 4. Temporal Splitting for Time Series Datadef preprocess_data(df):
    """
    Comprehensive data preprocessing function
    """
    df_processed = df.copy()
    
    print("Starting data preprocessing...")
    
    # 1. Handle problematic gender values (2 -> missing, to be imputed later)
    print(f"Gender value 2 count before cleaning: {(df_processed['gender'] == 2).sum()}")
    df_processed.loc[df_processed['gender'] == 2, 'gender'] = np.nan
    print(f"Gender value 2 converted to NaN")
    
    # 2. Clip age to reasonable bounds
    df_processed['age'] = df_processed['age'].clip(18, 80)
    
    # 3. Create additional time-based features
    df_processed['case_open_date'] = pd.to_datetime(df_processed['case_open_date'])
    df_processed['year'] = df_processed['case_open_date'].dt.year
    df_processed['month'] = df_processed['case_open_date'].dt.month
    df_processed['quarter'] = df_processed['case_open_date'].dt.quarter
    df_processed['day_of_week'] = df_processed['case_open_date'].dt.dayofweek
    
    # 4. Create derived features
    df_processed['total_requests'] = (df_processed['visa_request_count'] + 
                                    df_processed['residence_request_count'] + 
                                    df_processed['visa_visits_request_count'])
    
    # Total ICCS cases
    iccs_columns = [col for col in df_processed.columns if 'Case_count_ICCS' in col]
    df_processed['total_iccs_cases'] = df_processed[iccs_columns].sum(axis=1)
    
    # 5. Define categorical and numerical columns
    categorical_features = [
        'education_level_code', 'marital_status', 'relegion_code', 
        'case_place_emi_code', 'nationality_code_curr_nat', 'isCitizen',
        'gender', 'year', 'quarter', 'day_of_week'
    ]
    
    numerical_features = [
        'age', 'visa_request_count', 'residence_request_count', 
        'visa_visits_request_count', 'total_requests', 'total_iccs_cases', 'month'
    ] + iccs_columns
    
    print(f"Identified {len(categorical_features)} categorical features")
    print(f"Identified {len(numerical_features)} numerical features")
    
    return df_processed, categorical_features, numerical_features

# Apply preprocessing
df_processed, categorical_features, numerical_features = preprocess_data(df)

print(f"\nPreprocessing completed!")
print(f"Final dataset shape: {df_processed.shape}")## 3. Data Preprocessing and Cleaning# Display first few rows and basic info
print("First 5 rows of the dataset:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nMissing Values:")
print(df.isnull().sum())# Set random seed for reproducibility
np.random.seed(42)

# Create sample dataset based on provided structure
def create_sample_dataset(n_samples=10000):
    """
    Create a sample dataset matching the provided structure
    """
    
    # Generate date range from 2020-01-01 to 2025-05-14
    start_date = datetime(2020, 1, 1)
    end_date = datetime(2025, 5, 14)
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    
    data = {
        'person_id': np.random.randint(100000, 999999, n_samples),
        'case_id': np.random.randint(10000, 99999, n_samples),
        'case_open_date': np.random.choice(date_range, n_samples),
        'education_level_code': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.1, 0.2, 0.3, 0.3, 0.1]),
        'marital_status': np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.4, 0.2, 0.1]),
        'age': np.random.normal(35, 12, n_samples).astype(int),
        'gender': np.random.choice([0, 1, 2], n_samples, p=[0.48, 0.47, 0.05]),  # Include problematic value 2
        'relegion_code': np.random.choice([1, 2, 3, 4, 5], n_samples),
        'case_place_emi_code': np.random.choice(range(1, 50), n_samples),
        'nationality_code_curr_nat': np.random.choice(range(1, 200), n_samples),
        'visa_request_count': np.random.poisson(2, n_samples),
        'residence_request_count': np.random.poisson(1, n_samples),
        'visa_visits_request_count': np.random.poisson(3, n_samples),
        'isCitizen': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),
    }
    
    # Add ICCS case counts (11 columns)
    for i in range(1, 12):
        if i == 10:
            col_name = f'Case_count_ICCS_102'
        elif i == 2:
            col_name = f'Case_count_ICCS_22'
        elif i == 3:
            col_name = f'Case_count_ICCS_33'
        else:
            col_name = f'Case_count_ICCS_{i}'
        data[col_name] = np.random.poisson(0.5, n_samples)
    
    # Create target variable with imbalance ratio of 1.5 (risk=1 : non-risk=0 = 1:1.5)
    # This means ~40% risk, 60% non-risk
    data['isRisk'] = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])
    
    # Create DataFrame
    df = pd.DataFrame(data)
    
    # Clip age to reasonable bounds
    df['age'] = df['age'].clip(18, 80)
    
    # Introduce some missing values randomly
    missing_columns = ['education_level_code', 'marital_status', 'relegion_code']
    for col in missing_columns:
        missing_mask = np.random.random(len(df)) < 0.05  # 5% missing
        df.loc[missing_mask, col] = np.nan
    
    return df

# Create the dataset
df = create_sample_dataset(10000)

print(f"Dataset created with shape: {df.shape}")
print(f"\nColumns: {list(df.columns)}")
print(f"\nTarget distribution:")
print(df['isRisk'].value_counts(normalize=True))
print(f"\nActual imbalance ratio: {df['isRisk'].value_counts()[0] / df['isRisk'].value_counts()[1]:.2f}")## 2. Create Sample Dataset

Based on the provided structure, let's create a comprehensive sample dataset for development and testing.# Data manipulation and analysis
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8')

# Machine Learning
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Metrics and Evaluation
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, precision_recall_curve, average_precision_score,
    accuracy_score, precision_score, recall_score, f1_score
)

# Imbalanced learning
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

# Feature selection
from sklearn.feature_selection import SelectKBest, f_classif, RFE

print("All libraries imported successfully!")## 1. Import Required Libraries# Risk Prediction Machine Learning Model
## Senior Data Science Project

This notebook implements a comprehensive machine learning pipeline to predict risk with probability scores.

### Project Overview:
- **Objective**: Predict if a person will be a risk (binary classification) with probability scores
- **Dataset Period**: 1/1/2020 to 14/05/2025
- **Class Imbalance Ratio**: 1.5
- **Approach**: Time series aware splitting with proper preprocessing

### Key Features:
- Comprehensive data preprocessing and cleaning
- Temporal splitting for time series data
- Handling categorical features and missing values
- Class imbalance handling
- Cross-validation and hyperparameter tuning
- Model evaluation with probability scores